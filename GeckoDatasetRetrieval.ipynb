{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeckoDatasetRetrieval.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPp0SO4kHRc30zEud/wN9jZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandonli/Gecko-Binary-Classifier/blob/main/GeckoDatasetRetrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FPZjeTtuxdh"
      },
      "source": [
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA_z07i5VoPO"
      },
      "source": [
        "import requests, re, time\n",
        "import torch, torchvision\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse, imghdr, pickle, posixpath, re\n",
        "import signal, socket, threading\n",
        "import urllib.parse, urllib.request\n",
        "import datetime, os, sys, logging, hashlib\n",
        "from pathlib import Path\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "%mkdir -p data/gold_dust_day_gecko\n",
        "%mkdir -p data/giant_day_gecko"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nivf-Dau-ol"
      },
      "source": [
        "The following code comes from https://github.com/ostrolucky/Bulk-Bing-Image-downloader, and was used to gather the dataset. It has been adapted for the purposes of this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzrTb0ScUbfL"
      },
      "source": [
        "output_dir = '/content/data/giant_day_gecko' # When downloading other dataset, comment this line and uncomment the line below\n",
        "#output_dir = '/content/data/gold_dust_day_gecko'\n",
        "\n",
        "\n",
        "adult_filter = True  # Do not disable adult filter by default\n",
        "socket.setdefaulttimeout(2)\n",
        "\n",
        "tried_urls = []\n",
        "image_md5s = {}\n",
        "in_progress = 0\n",
        "urlopenheader = {'User-Agent': 'Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0'}\n",
        "\n",
        "def download(pool_sema: threading.Semaphore, img_sema: threading.Semaphore, url: str, output_dir: str, limit: int):\n",
        "    global in_progress\n",
        "\n",
        "    if url in tried_urls:\n",
        "        print('SKIP: Already checked url, skipping')\n",
        "        return\n",
        "    pool_sema.acquire()\n",
        "    in_progress += 1\n",
        "    acquired_img_sema = False\n",
        "    path = urllib.parse.urlsplit(url).path\n",
        "    filename = posixpath.basename(path).split('?')[0]  # Strip GET parameters from filename\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    name = name[:36].strip()\n",
        "    filename = name + ext\n",
        "\n",
        "    try:\n",
        "        request = urllib.request.Request(url, None, urlopenheader)\n",
        "        image = urllib.request.urlopen(request).read()\n",
        "        if not imghdr.what(None, image):\n",
        "            print('SKIP: Invalid image, not saving ' + filename)\n",
        "            return\n",
        "\n",
        "        md5_key = hashlib.md5(image).hexdigest()\n",
        "        if md5_key in image_md5s:\n",
        "            print('SKIP: Image is a duplicate of ' + image_md5s[md5_key] + ', not saving ' + filename)\n",
        "            return\n",
        "\n",
        "        i = 0\n",
        "        while os.path.exists(os.path.join(output_dir, filename)):\n",
        "            if hashlib.md5(open(os.path.join(output_dir, filename), 'rb').read()).hexdigest() == md5_key:\n",
        "                print('SKIP: Already downloaded ' + filename + ', not saving')\n",
        "                return\n",
        "            i += 1\n",
        "            filename = \"%s-%d%s\" % (name, i, ext)\n",
        "\n",
        "        image_md5s[md5_key] = filename\n",
        "\n",
        "        img_sema.acquire()\n",
        "        acquired_img_sema = True\n",
        "        if limit is not None and len(tried_urls) >= limit:\n",
        "            return\n",
        "\n",
        "        imagefile = open(os.path.join(output_dir, filename), 'wb')\n",
        "        imagefile.write(image)\n",
        "        imagefile.close()\n",
        "        print(\" OK : \" + filename)\n",
        "        tried_urls.append(url)\n",
        "    except Exception as e:\n",
        "        print(\"FAIL: \" + filename)\n",
        "    finally:\n",
        "        pool_sema.release()\n",
        "        if acquired_img_sema:\n",
        "            img_sema.release()\n",
        "        in_progress -= 1\n",
        "\n",
        "def fetch_images_from_keyword(pool_sema: threading.Semaphore, img_sema: threading.Semaphore, keyword: str,\n",
        "                              output_dir: str, filters: str, limit: int):\n",
        "    current = 0\n",
        "    last = ''\n",
        "    while True:\n",
        "        time.sleep(0.1)\n",
        "\n",
        "        if in_progress > 10:\n",
        "            continue\n",
        "\n",
        "        request_url = 'https://www.bing.com/images/async?q=' + urllib.parse.quote_plus(keyword) + '&first=' + str(\n",
        "            current) + '&count=35&adlt=' + adlt + '&qft=' + ('' if filters is None else filters)\n",
        "        request = urllib.request.Request(request_url, None, headers=urlopenheader)\n",
        "        response = urllib.request.urlopen(request)\n",
        "        html = response.read().decode('utf8')\n",
        "        links = re.findall('murl&quot;:&quot;(.*?)&quot;', html)\n",
        "        try:\n",
        "            if links[-1] == last:\n",
        "                return\n",
        "            for index, link in enumerate(links):\n",
        "                if limit is not None and len(tried_urls) >= limit:\n",
        "                    return\n",
        "                t = threading.Thread(target=download, args=(pool_sema, img_sema, link, output_dir, limit))\n",
        "                t.start()\n",
        "                current += 1\n",
        "            last = links[-1]\n",
        "        except IndexError:\n",
        "            print('FAIL: No search results for \"{0}\"'.format(keyword))\n",
        "            return\n",
        "\n",
        "def backup_history(*args):\n",
        "    download_history = open(os.path.join(output_dir, 'download_history.pickle'), 'wb')\n",
        "    pickle.dump(tried_urls, download_history)\n",
        "    copied_image_md5s = dict(\n",
        "        image_md5s)  # We are working with the copy, because length of input variable for pickle must not be changed during dumping\n",
        "    pickle.dump(copied_image_md5s, download_history)\n",
        "    download_history.close()\n",
        "    print('history_dumped')\n",
        "    if args:\n",
        "        exit(0)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='Bing image bulk downloader')\n",
        "    parser.add_argument('-s', '--search-string', help='Keyword to search', required=False)\n",
        "    parser.add_argument('-f', '--search-file', help='Path to a file containing search strings line by line',\n",
        "                        required=False)\n",
        "    parser.add_argument('-o', '--output', help='Output directory', required=False)\n",
        "    parser.add_argument('--adult-filter-on', help='Enable adult filter', action='store_true', required=False)\n",
        "    parser.add_argument('--adult-filter-off', help='Disable adult filter', action='store_true', required=False)\n",
        "    parser.add_argument('--filters',\n",
        "                        help='Any query based filters you want to append when searching for images, e.g. +filterui:license-L1',\n",
        "                        required=False)\n",
        "    parser.add_argument('--limit', help='Make sure not to search for more than specified amount of images.',\n",
        "                        required=False, type=int)\n",
        "    parser.add_argument('--threads', help='Number of threads', type=int, default=20)\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "\n",
        "\n",
        "    args.limit = 700;\n",
        "    args.search_string = \"Giant Day Gecko\" # When downloading other dataset, comment this line and uncomment the line below\n",
        "    #args.search_string = \"Gold Dust Day Gecko\"\n",
        "\n",
        "\n",
        "\n",
        "    if (not args.search_string) and (not args.search_file):\n",
        "        parser.error('Provide Either search string or path to file containing search strings')\n",
        "    if args.output:\n",
        "        output_dir = args.output\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    output_dir_origin = output_dir\n",
        "    signal.signal(signal.SIGINT, backup_history)\n",
        "    try:\n",
        "        download_history = open(os.path.join(output_dir, 'download_history.pickle'), 'rb')\n",
        "        tried_urls = pickle.load(download_history)\n",
        "        image_md5s = pickle.load(download_history)\n",
        "        download_history.close()\n",
        "    except (OSError, IOError):\n",
        "        tried_urls = []\n",
        "    if adult_filter:\n",
        "        adlt = ''\n",
        "    else:\n",
        "        adlt = 'off'\n",
        "    if args.adult_filter_off:\n",
        "        adlt = 'off'\n",
        "    elif args.adult_filter_on:\n",
        "        adlt = ''\n",
        "    pool_sema = threading.BoundedSemaphore(args.threads)\n",
        "    img_sema = threading.Semaphore()\n",
        "    if args.search_string:\n",
        "        fetch_images_from_keyword(pool_sema, img_sema, args.search_string, output_dir, args.filters, args.limit)\n",
        "    elif args.search_file:\n",
        "        try:\n",
        "            inputFile = open(args.search_file)\n",
        "        except (OSError, IOError):\n",
        "            print(\"FAIL: Couldn't open file {}\".format(args.search_file))\n",
        "            exit(1)\n",
        "        for keyword in inputFile.readlines():\n",
        "            output_sub_dir = os.path.join(output_dir_origin, keyword.strip().replace(' ', '_'))\n",
        "            if not os.path.exists(output_sub_dir):\n",
        "                os.makedirs(output_sub_dir)\n",
        "            fetch_images_from_keyword(pool_sema, keyword, output_sub_dir, args.filters, args.limit)\n",
        "            backup_history()\n",
        "            time.sleep(10)\n",
        "        inputFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5gzxw9_vWK0"
      },
      "source": [
        "I chose to download 700 images from each as a way to overcome duplicates/bad images/image scraper not downloading up to the limit. The following code comes from https://github.com/KiranKumarChilla/Removing-Duplicate-Docs-Using-Hashing-in-Python and was used to remove any exact duplicates in the dataset. The code was modified for the purposes of this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEIpv9O9Cba-"
      },
      "source": [
        "input_files_path = r'/content/data/giant_day_gecko' # When removing duplicates from other dataset,\n",
        "#input_files_path = r'/content/data/gold_dust_day_gecko'        # comment this line and uncomment the line below\n",
        "\n",
        "input_files = []\n",
        "input_files = [f for f in listdir(input_files_path) if isfile(join(input_files_path, f))]\n",
        "input_files = [os.path.join(input_files_path, x) for x in input_files]\n",
        "inp_dups = {}\n",
        "unique_inps = {}\n",
        "\n",
        "# It calculates the hash value for each file ; decrease the block size if input file size is more\n",
        "def calculate_hash_val(path, blocksize=65536):\n",
        "    afile = open(path, 'rb')\n",
        "    hasher = hashlib.md5()\n",
        "    buf = afile.read()\n",
        "    while len(buf) > 0:\n",
        "        hasher.update(buf)\n",
        "        buf = afile.read()\n",
        "    afile.close()\n",
        "    return hasher.hexdigest()\n",
        "\n",
        "# Joins two dictionaries\n",
        "def find_dups(dic_unique, dict1, dict2={}):\n",
        "    for key in dict1.keys():\n",
        "        if key not in dict2 and key not in dic_unique:\n",
        "            dic_unique[key] = dict1[key]\n",
        "\n",
        "# Identifying unique files\n",
        "def find_unique_files(dic_unique, dict1):\n",
        "    for key in dict1.keys():\n",
        "        if key not in dic_unique:\n",
        "            dic_unique[key] = dict1[key]\n",
        "\n",
        "def remove_duplicate_files(all_inps, unique_inps):\n",
        "    for file_name in all_inps.keys():\n",
        "        if all_inps[file_name] in unique_inps and file_name!=unique_inps[all_inps[file_name]]:\n",
        "            os.remove(file_name)\n",
        "        elif all_inps[file_name] not in unique_inps:\n",
        "            os.remove(file_name)\n",
        "\n",
        "# main function in this file which calls all other function and process inputs\n",
        "def rmv_dup_process(input_files):\n",
        "    all_inps={}\n",
        "\n",
        "    for file_path in input_files:\n",
        "        if Path(file_path).exists():\n",
        "           files_hash = calculate_hash_val(file_path)\n",
        "           inp_dups[files_hash]=file_path\n",
        "           all_inps[file_path] = files_hash\n",
        "        else:\n",
        "            print('%s is not a valid path, please verify' % file_path)\n",
        "            sys.exit()\n",
        "\n",
        "    find_unique_files(unique_inps, inp_dups)\n",
        "    remove_duplicate_files(all_inps, unique_inps)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rmv_dup_process(input_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLoQlEH_vwMx"
      },
      "source": [
        "The next two cells were used to see the size of my datasets and to mount to drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6ER2SzUHQJe"
      },
      "source": [
        "img_folder_path = '/content/data/giant_day_gecko'\n",
        "dirListing = os.listdir(img_folder_path)\n",
        "\n",
        "img_folder_path2 = '/content/data/gold_dust_day_gecko'\n",
        "dirListing2 = os.listdir(img_folder_path2)\n",
        "\n",
        "print('Giant Day Gecko Images: ' + str(len(dirListing)))\n",
        "print('Gold Dust Day Gecko Images: ' + str(len(dirListing2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so9wsmY1iQXs",
        "outputId": "e3d755d0-d5cb-4d97-aee2-0b1026427747"
      },
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}